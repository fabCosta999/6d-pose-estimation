{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkzR3dDS3nWm"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J8yUdpQtNjb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import yaml\n",
        "import trimesh\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptCcdv6XyMnM"
      },
      "source": [
        "*Pretrained Models and Dataset Paths*\n",
        "\n",
        "This notebook assumes that all models have already been trained.\n",
        "The following paths point to pretrained weights and dataset locations and are provided as placeholders for demonstration purposes.\n",
        "\n",
        "Please replace each `path/to/..`. entry with the corresponding local path before running the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i1lxu6IqrdO"
      },
      "outputs": [],
      "source": [
        "yolo_weights_path = \"path/to/yolo_weights.pt\"\n",
        "rotation_extension_weights_path = \"path/to/rot_ext_weights.pth\"\n",
        "translation_extension_weights_path = \"path/to/trans_ext_weights.pth\"\n",
        "yolo_dataset = \"path/to/yolo/data.yaml\"\n",
        "data_root = \"path/to/dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJGrHXcZyi_d"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KtscGS_yvoy"
      },
      "source": [
        "*RGB-D Rotation Estimation Network*\n",
        "\n",
        "The following model implements a late-fusion architecture for rotation estimation using both RGB and depth information.\n",
        "\n",
        "The RGB branch is based on a ResNet-50 encoder pretrained on ImageNet.\n",
        "\n",
        "The depth branch is a lightweight convolutional network that processes single-channel depth maps.\n",
        "\n",
        "Features from both modalities are concatenated and passed through a fully connected head to predict a 4D unit quaternion representing object rotation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPigNX-yq-vx"
      },
      "outputs": [],
      "source": [
        "class DepthNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DepthNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=2, padding=2); self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1); self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1); self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1); self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1); self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.pool(x)\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class DepthRotationNet(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(DepthRotationNet, self).__init__()\n",
        "\n",
        "        try:\n",
        "            weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n",
        "            base_resnet = models.resnet50(weights=weights)\n",
        "        except:\n",
        "            base_resnet = models.resnet50(pretrained=pretrained)\n",
        "\n",
        "        self.rgb_encoder = nn.Sequential(*list(base_resnet.children())[:-1])\n",
        "        self.depth_encoder = DepthNet()\n",
        "\n",
        "        # Fusion & Heads\n",
        "        self.fc1 = nn.Linear(2048 + 512, 1024)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "\n",
        "        # Output Head\n",
        "        self.head = nn.Linear(1024, 4)  # Quaternion (4D)\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        f_rgb = self.rgb_encoder(rgb).view(rgb.size(0), -1)\n",
        "        f_depth = self.depth_encoder(depth)\n",
        "\n",
        "        # Concatenate features\n",
        "        f_fused = torch.cat((f_rgb, f_depth), dim=1)\n",
        "\n",
        "        x = F.relu(self.fc1(f_fused))\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return F.normalize(self.head(x), p=2, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teMnPw8PyyKL"
      },
      "source": [
        "*Encoder-Decoder Network for Translation Estimation*\n",
        "\n",
        "This encoder-decoder architecture predicts a dense per-pixel weight map used for translation estimation.\n",
        "\n",
        "The input consists of concatenated RGB and depth information (6 channels in total).\n",
        "\n",
        "The output is a single-channel, unnormalized weight map that is later used to aggregate pixel-level information for estimating object translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSsLf1u6rbDV"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderWeightsNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # ----- Encoder -----\n",
        "        self.conv1 = nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=1); self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.down1 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1); self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.down2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1); self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.down3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1); self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # ----- Decoder -----\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1); self.bn5 = nn.BatchNorm2d(64)\n",
        "        self.up2 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1); self.bn6 = nn.BatchNorm2d(32)\n",
        "        self.up3 = nn.ConvTranspose2d(64, 16, kernel_size=4, stride=2, padding=1); self.bn7 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # Output: 1 unnormalized weight per pixel\n",
        "        self.out_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ----- Encoder -----\n",
        "        x1 = F.relu(self.bn1(self.conv1(x)))    # 64x64x16\n",
        "        x2 = F.relu(self.bn2(self.down1(x1)))   # 32x32x32\n",
        "        x3 = F.relu(self.bn3(self.down2(x2)))   # 16x16x64\n",
        "        x4 = F.relu(self.bn4(self.down3(x3)))   # 8x8x128\n",
        "\n",
        "        # ----- Decoder -----\n",
        "        u1 = F.relu(self.bn5(self.up1(x4)))     # 16x16x64\n",
        "        u1 = torch.cat([u1, x3], dim=1)         # 16x16x128\n",
        "        u2 = F.relu(self.bn6(self.up2(u1)))     # 32x32x32\n",
        "        u2 = torch.cat([u2, x2], dim=1)         # 32x32x64\n",
        "        u3 = F.relu(self.bn7(self.up3(u2)))     # 64x64x16\n",
        "        w = self.out_conv(u3)                   # 64x64x1\n",
        "        return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhjWhz6bz_tH"
      },
      "source": [
        "# Utility Functions\n",
        "\n",
        "The following section contains a collection of low-level utility functions used throughout the pipeline.\n",
        "\n",
        "These utilities handle:\n",
        "- object symmetries (LineMOD-specific),\n",
        "- quaternion and rotation algebra,\n",
        "- 3D point cloud metrics,\n",
        "- spatial grids and soft attention,\n",
        "- pinhole camera geometry for depth-based translation estimation.\n",
        "\n",
        "You can safely skip this entire section when reading the notebook.\n",
        "The functions are provided here only to keep the notebook fully self-contained and runnable without external imports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4H36THx0Rph"
      },
      "source": [
        "*Object Symmetries (LineMOD)*\n",
        "\n",
        "Some LineMOD objects exhibit discrete rotational symmetries (e.g., eggbox, glue).\n",
        "\n",
        "The following definitions specify:\n",
        "- which objects are symmetric,\n",
        "- the set of equivalent quaternions used during evaluation.\n",
        "\n",
        "These symmetries are taken into account when computing rotation errors to avoid penalizing physically equivalent poses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWxSvs6CtErF"
      },
      "outputs": [],
      "source": [
        "class SymmetryType(Enum):\n",
        "    NONE = 0\n",
        "    DISCRETE = 1\n",
        "\n",
        "\n",
        "LINEMOD_SYMMETRIES = {\n",
        "    7: SymmetryType.DISCRETE,  # eggbox\n",
        "    8: SymmetryType.DISCRETE,  # glue\n",
        "}\n",
        "\n",
        "SYMMETRIC_QUATS = {\n",
        "    7: torch.tensor([\n",
        "        [1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1.],   # 180° z\n",
        "    ]),\n",
        "    8: torch.tensor([\n",
        "        [1., 0., 0., 0.],\n",
        "        [0., 0., 0., 1.],\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8hlvhynsT5s"
      },
      "outputs": [],
      "source": [
        "def rotation_matrix_to_quaternion(R):\n",
        "    trace = R.trace()\n",
        "\n",
        "    if trace > 0:\n",
        "        s = torch.sqrt(trace + 1.0) * 2\n",
        "        qw = 0.25 * s\n",
        "        qx = (R[2, 1] - R[1, 2]) / s\n",
        "        qy = (R[0, 2] - R[2, 0]) / s\n",
        "        qz = (R[1, 0] - R[0, 1]) / s\n",
        "    else:\n",
        "        if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n",
        "            s = torch.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n",
        "            qw = (R[2, 1] - R[1, 2]) / s\n",
        "            qx = 0.25 * s\n",
        "            qy = (R[0, 1] + R[1, 0]) / s\n",
        "            qz = (R[0, 2] + R[2, 0]) / s\n",
        "        elif R[1, 1] > R[2, 2]:\n",
        "            s = torch.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n",
        "            qw = (R[0, 2] - R[2, 0]) / s\n",
        "            qx = (R[0, 1] + R[1, 0]) / s\n",
        "            qy = 0.25 * s\n",
        "            qz = (R[1, 2] + R[2, 1]) / s\n",
        "        else:\n",
        "            s = torch.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n",
        "            qw = (R[1, 0] - R[0, 1]) / s\n",
        "            qx = (R[0, 2] + R[2, 0]) / s\n",
        "            qy = (R[1, 2] + R[2, 1]) / s\n",
        "            qz = 0.25 * s\n",
        "\n",
        "    q = torch.tensor([qw, qx, qy, qz], dtype=torch.float32)\n",
        "    return q / torch.norm(q)\n",
        "\n",
        "def quaternion_to_rotation_matrix(q):\n",
        "    q = q / q.norm()\n",
        "    w, x, y, z = q\n",
        "    return torch.tensor([\n",
        "        [1-2*(y*y+z*z), 2*(x*y-z*w),   2*(x*z+y*w)],\n",
        "        [2*(x*y+z*w),   1-2*(x*x+z*z), 2*(y*z-x*w)],\n",
        "        [2*(x*z-y*w),   2*(y*z+x*w),   1-2*(x*x+y*y)],\n",
        "    ], device=q.device)\n",
        "\n",
        "\n",
        "def quat_mul(q1, q2):\n",
        "    w1, x1, y1, z1 = q1.unbind(-1)\n",
        "    w2, x2, y2, z2 = q2.unbind(-1)\n",
        "\n",
        "    return torch.stack([\n",
        "        w1*w2 - x1*x2 - y1*y2 - z1*z2,\n",
        "        w1*x2 + x1*w2 + y1*z2 - z1*y2,\n",
        "        w1*y2 - x1*z2 + y1*w2 + z1*x2,\n",
        "        w1*z2 + x1*y2 - y1*x2 + z1*w2\n",
        "    ], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6ecIM_JtWHj"
      },
      "outputs": [],
      "source": [
        "def load_linemod_models(models_dir, device=\"cpu\"):\n",
        "    models = {}\n",
        "    for ply in Path(models_dir).glob(\"obj_*.ply\"):\n",
        "        obj_id = int(ply.stem.split(\"_\")[1])\n",
        "        mesh = trimesh.load(ply, process=False)\n",
        "        pts = torch.tensor(mesh.vertices, dtype=torch.float32, device=device)\n",
        "        models[obj_id] = pts\n",
        "    return models\n",
        "\n",
        "def add_metric(\n",
        "    model_points,   # (N,3)\n",
        "    R_pred, t_pred, # (3,3), (3,)\n",
        "    R_gt,   t_gt,   # (3,3), (3,)\n",
        "):\n",
        "    pts_pred = (R_pred @ model_points.T).T + t_pred\n",
        "    pts_gt   = (R_gt   @ model_points.T).T + t_gt\n",
        "\n",
        "    dists = torch.norm(pts_pred - pts_gt, dim=1)\n",
        "\n",
        "    return dists.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr8bfrMltzDN"
      },
      "outputs": [],
      "source": [
        "def build_uv_grid(box, H, W, device):\n",
        "    B = box.shape[0]\n",
        "\n",
        "    x, y, bw, bh = box[:, 0], box[:, 1], box[:, 2], box[:, 3]\n",
        "\n",
        "    i = torch.arange(H, device=device).float()\n",
        "    j = torch.arange(W, device=device).float()\n",
        "    ii, jj = torch.meshgrid(i, j, indexing=\"ij\")\n",
        "\n",
        "    ii = ii.unsqueeze(0).expand(B, -1, -1)\n",
        "    jj = jj.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "    u = x[:, None, None] + (jj + 0.5) * bw[:, None, None] / W\n",
        "    v = y[:, None, None] + (ii + 0.5) * bh[:, None, None] / H\n",
        "\n",
        "    return torch.stack([u, v], dim=-1)\n",
        "\n",
        "\n",
        "def spatial_softmax(weight_map, mask=None, tau=0.05):\n",
        "    B, _, H, W = weight_map.shape\n",
        "    w = weight_map.view(B, -1) / tau\n",
        "\n",
        "    if mask is not None:\n",
        "        m = mask.view(B, -1)\n",
        "        w = w.masked_fill(m == 0, -1e9)\n",
        "\n",
        "    w = torch.softmax(w, dim=1)\n",
        "    return w.view(B, 1, H, W)\n",
        "\n",
        "def make_coord_grid(H, W, device):\n",
        "    ys = torch.linspace(-1, 1, H, device=device)\n",
        "    xs = torch.linspace(-1, 1, W, device=device)\n",
        "    yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
        "    grid = torch.stack([xx, yy], dim=0)\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1txVfYG0jDT"
      },
      "source": [
        "*Pinhole Camera Model and Translation Estimation*\n",
        "\n",
        "The following utilities implement basic pinhole camera geometry.\n",
        "\n",
        "Depth values are back-projected into 3D space using camera intrinsics, and a learned per-pixel weight map is used to compute a weighted average of 3D points, yielding the final translation estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9xm8y0At9W9"
      },
      "outputs": [],
      "source": [
        "def depth_to_points(depth, K, uv_grid):\n",
        "    \"\"\"\n",
        "    depth:   [B, 1, H, W]\n",
        "    uv_grid: [B, H, W, 2]\n",
        "    K:       [3, 3]\n",
        "    return:  [B, H, W, 3]\n",
        "    \"\"\"\n",
        "    fx = K[0, 0]\n",
        "    fy = K[1, 1]\n",
        "    cx = K[0, 2]\n",
        "    cy = K[1, 2]\n",
        "\n",
        "    u = uv_grid[..., 0]\n",
        "    v = uv_grid[..., 1]\n",
        "    z = depth.squeeze(1)\n",
        "\n",
        "    x = (u - cx) * z / fx\n",
        "    y = (v - cy) * z / fy\n",
        "\n",
        "    return torch.stack([x, y, z], dim=-1)\n",
        "\n",
        "\n",
        "def weighted_translation(points_3d, weights):\n",
        "    weights = weights.permute(0, 2, 3, 1)\n",
        "    t = (points_3d * weights).sum(dim=(1,2))\n",
        "    return t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pipeline that goes from rgb and depth data to the translation prediction is wrapped inside the DepthTranslationNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DepthTranslationNet(nn.Module):\n",
        "    def __init__(self, depth_mean, depth_std):\n",
        "        super().__init__()\n",
        "        self.enc_dec = EncoderDecoderWeightsNet()\n",
        "        \n",
        "        self.register_buffer('depth_mean', torch.tensor(depth_mean))\n",
        "        self.register_buffer('depth_std', torch.tensor(depth_std))\n",
        "\n",
        "    def forward(self, rgb, depth, coord, box, K):\n",
        "        # 1. depth denormalization\n",
        "        un_normalized_depth = depth * self.depth_std + self.depth_mean\n",
        "\n",
        "        # 2. encoder-decoder input\n",
        "        x = torch.cat([rgb, depth, coord], dim=1)\n",
        "\n",
        "        # 3. encoder-decoder output\n",
        "        logits = self.enc_dec(x)  \n",
        "\n",
        "        # 4. pixel < 10 mm are considered backgroud\n",
        "        valid_mask = (un_normalized_depth > 10.0).float()\n",
        "        \n",
        "        # 5 weights computation\n",
        "        weights = spatial_softmax(logits, valid_mask)\n",
        "\n",
        "        # 4. Ricostruzione 3D (Inverse Pinhole)\n",
        "        B, _, H, W = depth.shape\n",
        "        device = depth.device\n",
        "        \n",
        "        # 5. u-v grid creation based on bounding box crop\n",
        "        uv_grid = build_uv_grid(box, H, W, device)\n",
        "\n",
        "        # 6. 2D -> 3D points projection\n",
        "        points_3d = depth_to_points(un_normalized_depth, K, uv_grid)\n",
        "\n",
        "        # 7. translation regression\n",
        "        t_pred = weighted_translation(points_3d, weights)\n",
        "\n",
        "        return weights, t_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN9PbE7O1VXs"
      },
      "source": [
        "# Dataset\n",
        "This dataset wrapper provides access to RGB, depth, camera intrinsics and ground-truth 6D pose\n",
        "information from the preprocessed LineMOD dataset.\n",
        "\n",
        "**Key assumptions and design choices:**\n",
        "- The dataset is expected to follow the standard `Linemod_preprocessed` structure.\n",
        "- A **per-object random train/test split** is performed at initialization.\n",
        "- Bounding boxes are taken directly from the ground-truth annotations.\n",
        "- Camera intrinsics are assumed to be shared across all objects and images.\n",
        "\n",
        "> **Note:**  \n",
        "> This class is provided for completeness and reproducibility.  \n",
        "> You can safely skip reading its implementation and treat it as a black box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFv4JDgSreyN"
      },
      "outputs": [],
      "source": [
        "class LinemodSceneDataset(Dataset):\n",
        "    CLASSES = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "    OBJ_ID_TO_CLASS = {obj_id: i for i, obj_id in enumerate(CLASSES)}\n",
        "\n",
        "    def __init__(self, dataset_root, split=\"train\", split_ratio=0.8, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.dataset_root = Path(dataset_root)\n",
        "        self.split = split\n",
        "        self.samples = []\n",
        "        self.gt_data = {}\n",
        "\n",
        "        for obj_id in self.CLASSES:\n",
        "            obj_dir = self.dataset_root / \"data\" / f\"{obj_id:02d}\"\n",
        "\n",
        "            rgb_dir = obj_dir / \"rgb\"\n",
        "            num_images = len(list(rgb_dir.glob(\"*.png\")))\n",
        "\n",
        "\n",
        "            indexes = np.arange(num_images)\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "            split_point = int(split_ratio * num_images)\n",
        "            if split == \"train\":\n",
        "                img_ids = indexes[:split_point]\n",
        "            else:\n",
        "                img_ids = indexes[split_point:]\n",
        "\n",
        "            for img_id in img_ids:\n",
        "                self.samples.append((obj_id, img_id))\n",
        "\n",
        "            with open(obj_dir / \"gt.yml\") as f:\n",
        "                self.gt_data[obj_id] = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "        any_obj = self.CLASSES[0]\n",
        "        info_path = self.dataset_root / \"data\" / f\"{any_obj:02d}\" / \"info.yml\"\n",
        "\n",
        "        with open(info_path) as f:\n",
        "            info = yaml.safe_load(f)\n",
        "\n",
        "        cam_info = next(iter(info.values()))\n",
        "\n",
        "        self.K = torch.tensor(cam_info[\"cam_K\"], dtype=torch.float32).view(3, 3)\n",
        "        self.depth_scale = cam_info.get(\"depth_scale\", 1.0)\n",
        "        self.rgb_transform = T.ToTensor()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obj_id, img_id = self.samples[idx]\n",
        "\n",
        "        base_dir = self.dataset_root / \"data\" / f\"{obj_id:02d}\"\n",
        "\n",
        "        img_path = base_dir / \"rgb\" / f\"{img_id:04d}.png\"\n",
        "        depth_path = base_dir / \"depth\" / f\"{img_id:04d}.png\"\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        W, H = img.size\n",
        "\n",
        "        rgb = self.rgb_transform(img)\n",
        "\n",
        "        object = None\n",
        "        for entry in self.gt_data[obj_id][img_id]:\n",
        "            if int(entry[\"obj_id\"]) == obj_id:\n",
        "                object = entry\n",
        "                break\n",
        "\n",
        "        if object is None:\n",
        "            raise RuntimeError(\n",
        "                f\"Object {obj_id} not found in image {img_id}\"\n",
        "            )\n",
        "\n",
        "        R = torch.tensor(object[\"cam_R_m2c\"], dtype=torch.float32).view(3, 3)\n",
        "        q = rotation_matrix_to_quaternion(R)\n",
        "        t = torch.tensor(object[\"cam_t_m2c\"], dtype=torch.float32).view(3)\n",
        "\n",
        "        return {\n",
        "            \"img_path\": img_path,\n",
        "            \"depth_path\": depth_path,\n",
        "            \"cam_intrinsics\": self.K,\n",
        "            \"rgb\": rgb,\n",
        "            \"bbox\": object[\"obj_bb\"],\n",
        "            \"label\": self.OBJ_ID_TO_CLASS[obj_id],\n",
        "            \"rotation\": q,\n",
        "            \"translation\":t,\n",
        "            \"size\": (W, H),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_6VlD-B3AZt"
      },
      "source": [
        "# End-to-End Evaluation Loop\n",
        "\n",
        "The following cells run the **full 6D pose estimation pipeline** on the LineMOD test set:\n",
        "\n",
        "1. Object detection with **YOLO**\n",
        "2. Rotation estimation using the **RGB-D fusion network**\n",
        "3. Translation estimation via **encoder-decoder weighted depth aggregation**\n",
        "4. Evaluation with **ADD-S**, including symmetry handling\n",
        "\n",
        "Basic logging is included to track detection failures and invalid depth cases, but only aggregate ADD-S statistics are reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-TckjmeuCEx"
      },
      "outputs": [],
      "source": [
        "resnet_tf = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "def crop_rgb(img_pil, bbox):\n",
        "    x, y, w, h = bbox\n",
        "    if w <= 1 or h <= 1:\n",
        "        return None\n",
        "    x1 = int(max(0, x))\n",
        "    y1 = int(max(0, y))\n",
        "    x2 = int(min(img_pil.width,  x + w))\n",
        "    y2 = int(min(img_pil.height, y + h))\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        return None\n",
        "    return img_pil.crop((x1, y1, x2, y2))\n",
        "\n",
        "def bbox_invalid(bbox):\n",
        "    x, y, w, h = bbox\n",
        "    return (w <= 1) or (h <= 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9U0y1WTsyfj"
      },
      "outputs": [],
      "source": [
        "log = defaultdict(lambda: {\n",
        "    \"adds\": [],\n",
        "    \"bbox_missing\": 0,\n",
        "    \"false_positive\": 0,\n",
        "    \"bbox_invalid\": 0,\n",
        "    \"depth_missing\": 0,\n",
        "    \"total\": 0,\n",
        "})\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# linemod depth mean and std for un-normalization\n",
        "depth_mean = 990.7\n",
        "depth_std  = 311.8\n",
        "\n",
        "# YOLO\n",
        "yolo = YOLO(yolo_weights_path)\n",
        "\n",
        "# RGBD (rotation)\n",
        "rotation_extension_net = DepthRotationNet(pretrained=False).to(device)\n",
        "rotation_extension_net.load_state_dict(torch.load(rotation_extension_weights_path, map_location=device))\n",
        "rotation_extension_net.eval()\n",
        "\n",
        "# EncDec (translation)\n",
        "translation_extension_net = DepthTranslationNet(depth_mean, depth_std).to(device)\n",
        "translation_extension_net.enc_dec.load_state_dict(torch.load(translation_extension_weights_path, map_location=device))\n",
        "translation_extension_net.eval()\n",
        "\n",
        "ds = LinemodSceneDataset(data_root, split=\"test\")\n",
        "models_3d = load_linemod_models(\n",
        "    f\"{data_root}/models\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "errors = []\n",
        "\n",
        "results = yolo.predict(\n",
        "        source=f\"{yolo_dataset}/test/images\",\n",
        "        imgsz=640,\n",
        "        batch=16,\n",
        "        device=device,\n",
        "        stream=False,\n",
        "        save=False,\n",
        "    )\n",
        "\n",
        "for r, scene in zip(results, ds):\n",
        "   # ---------------------------\n",
        "   # GT\n",
        "   # ---------------------------\n",
        "   q_gt = scene[\"rotation\"].to(device)\n",
        "   t_gt = scene[\"translation\"].to(device)\n",
        "   obj_class = scene[\"label\"]\n",
        "   obj_id = LinemodSceneDataset.CLASSES[obj_class]\n",
        "   log[obj_id][\"total\"] += 1\n",
        "\n",
        "   # ---------------------------\n",
        "   # YOLO bbox\n",
        "   # ---------------------------\n",
        "   boxes = r.boxes\n",
        "\n",
        "   if boxes is None or len(boxes) == 0:\n",
        "      log[obj_id][\"bbox_missing\"] += 1\n",
        "      continue\n",
        "\n",
        "   xyxy = boxes.xyxy\n",
        "   cls  = boxes.cls.long()\n",
        "   conf = boxes.conf\n",
        "   mask = cls == obj_class\n",
        "   if mask.sum() == 0:\n",
        "        log[obj_id][\"bbox_missing\"] += 1\n",
        "        log[obj_id][\"false_positive\"] += len(cls)\n",
        "        continue\n",
        "\n",
        "   log[obj_id][\"false_positive\"] += int((~mask).sum())\n",
        "   if mask.sum() > 1:\n",
        "      log[obj_id][\"false_positive\"] += int(mask.sum() - 1)\n",
        "\n",
        "   idxs = torch.where(mask)[0]\n",
        "   best = idxs[conf[idxs].argmax()]\n",
        "\n",
        "   x1, y1, x2, y2 = xyxy[best]\n",
        "\n",
        "   bbox = (\n",
        "      x1.item(),\n",
        "      y1.item(),\n",
        "      (x2 - x1).item(),\n",
        "      (y2 - y1).item(),\n",
        "   )\n",
        "\n",
        "   if bbox_invalid(bbox):\n",
        "      log[obj_id][\"bbox_invalid\"] += 1\n",
        "      continue\n",
        "\n",
        "   # ---------------------------\n",
        "   # LOAD RGB + DEPTH\n",
        "   # ---------------------------\n",
        "   img = Image.open(scene[\"img_path\"]).convert(\"RGB\")\n",
        "   depth_img = Image.open(scene[\"depth_path\"])\n",
        "\n",
        "   # ---------------------------\n",
        "   # CROP\n",
        "   # ---------------------------\n",
        "   crop_rgb_img = crop_rgb(img, bbox)\n",
        "   if crop_rgb_img is None:\n",
        "      log[obj_id][\"bbox_invalid\"] += 1\n",
        "      continue\n",
        "\n",
        "   x, y, w, h = bbox\n",
        "   x1 = int(max(0, x))\n",
        "   y1 = int(max(0, y))\n",
        "   x2 = int(min(depth_img.width,  x + w))\n",
        "   y2 = int(min(depth_img.height, y + h))\n",
        "   if x2 <= x1 or y2 <= y1:\n",
        "      log[obj_id][\"bbox_invalid\"] += 1\n",
        "      continue\n",
        "\n",
        "   crop_depth_img = depth_img.crop((x1, y1, x2, y2))\n",
        "\n",
        "   # =========================================================\n",
        "   # ROTATION — RGBD Fusion Net (224)\n",
        "   # =========================================================\n",
        "   rgb_224 = resnet_tf(crop_rgb_img).unsqueeze(0).to(device)\n",
        "\n",
        "   depth_224 = T.Compose([\n",
        "      T.Resize((224, 224), interpolation=T.InterpolationMode.NEAREST),\n",
        "      T.ToTensor(),\n",
        "   ])(crop_depth_img).unsqueeze(0).to(device)\n",
        "\n",
        "   depth_224 = (depth_224 - depth_mean) / depth_std\n",
        "\n",
        "   with torch.no_grad():\n",
        "      q_pred = rotation_extension_net(rgb_224, depth_224)[0]\n",
        "\n",
        "   R_pred = quaternion_to_rotation_matrix(q_pred)\n",
        "\n",
        "   # =========================================================\n",
        "   # TRANSLATION — Encoder Decoder (64)\n",
        "   # =========================================================\n",
        "   rgb_64 = T.Compose([\n",
        "      T.Resize((64, 64)),\n",
        "      T.ToTensor(),\n",
        "      T.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],\n",
        "          std =[0.229, 0.224, 0.225],\n",
        "      ),\n",
        "    ])(crop_rgb_img).unsqueeze(0).to(device)\n",
        "\n",
        "   depth_64 = T.Compose([\n",
        "      T.Resize((64, 64), interpolation=T.InterpolationMode.NEAREST),\n",
        "      T.ToTensor(),\n",
        "   ])(crop_depth_img).unsqueeze(0).to(device)\n",
        "\n",
        "   depth_64 = (depth_64 - depth_mean) / depth_std\n",
        "\n",
        "   coord = make_coord_grid(64, 64, device).unsqueeze(0)\n",
        "   box = torch.tensor(bbox, device=device).unsqueeze(0)\n",
        "   K = scene[\"cam_intrinsics\"].to(device)\n",
        "\n",
        "   with torch.no_grad():\n",
        "      _, t_pred = translation_extension_net(rgb_64, depth_64, coord, box, K)\n",
        "      t_pred = t_pred[0]\n",
        "\n",
        "   # =========================================================\n",
        "   # ADD-S\n",
        "   # =========================================================\n",
        "   pts = models_3d[obj_id]\n",
        "\n",
        "   if LINEMOD_SYMMETRIES.get(obj_class, SymmetryType.NONE) == SymmetryType.DISCRETE:\n",
        "      errs = []\n",
        "      for q_sym in SYMMETRIC_QUATS[obj_class]:\n",
        "          q_gt_sym = quat_mul(q_gt, q_sym.to(device))\n",
        "          R_gt_sym = quaternion_to_rotation_matrix(q_gt_sym)\n",
        "\n",
        "          errs.append(add_metric(\n",
        "                pts,\n",
        "                R_pred, t_pred,\n",
        "                R_gt_sym, t_gt,\n",
        "            ))\n",
        "\n",
        "      err = torch.stack(errs).min()\n",
        "   else:\n",
        "      R_gt = quaternion_to_rotation_matrix(q_gt)\n",
        "      err = add_metric(\n",
        "            pts,\n",
        "            R_pred, t_pred,\n",
        "            R_gt, t_gt,\n",
        "        )\n",
        "\n",
        "   log[obj_id][\"adds\"].append(err.item())\n",
        "   errors.append(err.item())\n",
        "\n",
        "\n",
        "errors = torch.tensor(errors)\n",
        "print(f\"ADD-S mean: {errors.mean():.2f} mm\")\n",
        "print(f\"ADD-S median: {errors.median():.2f} mm\")\n",
        "\n",
        "out_dir = \"results\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "csv_path = os.path.join(out_dir, \"eval_extension.csv\")\n",
        "adds_txt_path = os.path.join(out_dir, \"all_adds_extension.txt\")\n",
        "\n",
        "with open(csv_path, \"w\", newline=\"\") as f_csv, \\\n",
        "    open(adds_txt_path, \"w\") as f_txt:\n",
        "    writer = csv.writer(f_csv)\n",
        "    writer.writerow([\n",
        "            \"obj_id\",\n",
        "            \"num_samples\",\n",
        "            \"adds_mean_mm\",\n",
        "            \"adds_median_mm\",\n",
        "            \"bbox_missing\",\n",
        "            \"false_positive\",\n",
        "            \"bbox_invalid\",\n",
        "            \"depth_missing\",\n",
        "    ])\n",
        "\n",
        "    for obj_id, d in sorted(log.items()):\n",
        "        adds = np.array(d[\"adds\"])\n",
        "\n",
        "        # ---- CSV summary ----\n",
        "        writer.writerow([\n",
        "            obj_id,\n",
        "            d[\"total\"],\n",
        "            adds.mean() if len(adds) > 0 else np.nan,\n",
        "            np.median(adds) if len(adds) > 0 else np.nan,\n",
        "            d[\"bbox_missing\"],\n",
        "            d[\"false_positive\"],\n",
        "            d[\"bbox_invalid\"],\n",
        "            d[\"depth_missing\"],\n",
        "        ])\n",
        "\n",
        "        # ---- TXT: all errors ----\n",
        "        for e in adds:\n",
        "            f_txt.write(f\"{obj_id} {e:.6f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
